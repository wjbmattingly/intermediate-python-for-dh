{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators are a powerful concept in Python that can help us work with large amounts of text-based data more efficiently. In this tutorial, we will introduce generators, explain why they are important for memory management, and provide examples tailored to humanists.\n",
    "\n",
    "## Introduction to Iterators and Generators\n",
    "\n",
    "Before diving into generators, let's briefly introduce the concept of iterators. An iterator is an object that allows us to loop over a collection of items, such as a list or a string, one item at a time. Python has built-in iterator objects for many data structures like lists, tuples, and strings. \n",
    "\n",
    "A generator is a special type of iterator that allows us to generate a sequence of values on-the-fly, without having to store all the values in memory. Generators are created using a special type of function called a generator function. Instead of using the return keyword, generator functions use the `yield` keyword to `return` values one at a time.\n",
    "\n",
    "## Memory Management and the Importance of Generators\n",
    "\n",
    "As humanists, we work primarily with texts. Often, our texts are quite short, but when we want to work with large collections of documents, it can be challenging to hold all the data in your computer's memory.\n",
    "\n",
    "Memory, in the context of computers, refers to the temporary storage used by a computer to hold data that it is currently processing or has recently processed. Think of it like the desk space you use when working on a project. You can only have a limited number of items on the desk at once, and the more items you have, the harder it is to find and work with the ones you need.\n",
    "\n",
    "Computer memory, often called RAM (Random Access Memory), works in a similar way. It holds data and instructions that the computer needs to access quickly while performing tasks. The more data and instructions you try to store in memory, the more resources the computer needs to manage them, which can slow down the system.\n",
    "\n",
    "When working with large text-based data, memory management becomes crucial. Storing large amounts of data in memory can be inefficient and slow down your program. This is where generators come in handy.\n",
    "\n",
    "Generators allow you to process large datasets one item at a time, without loading the entire dataset into memory. This means that you can work with data that is too large to fit in memory, or process data more efficiently by only loading the necessary items.\n",
    "\n",
    "## Creating and Using Generators\n",
    "\n",
    "To create a generator function, use the `def` keyword to define a function, just like a regular function, but use the `yield` keyword instead of return to return values. Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_generator(text):\n",
    "    for word in text.split():\n",
    "        yield word\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generator function takes a text string as input and yields words one at a time. To use the generator, you need to create a generator object by calling the generator function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To\n",
      "be,\n",
      "or\n",
      "not\n",
      "to\n",
      "be,\n",
      "that\n",
      "is\n",
      "the\n",
      "question\n"
     ]
    }
   ],
   "source": [
    "text = \"To be, or not to be, that is the question\"\n",
    "word_gen = word_generator(text)\n",
    "\n",
    "for word in word_gen:\n",
    "    print(word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Example: Analyzing Literary Works\n",
    "\n",
    "Let's say you want to analyze the frequency of words in a large literary work, like \"War and Peace\" by Leo Tolstoy. With a generator, you can efficiently process the text without loading the entire book into memory.\n",
    "\n",
    "First, create a generator function to read the book line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_book_line_by_line(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            yield line\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a function to process the lines and count word frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_words(file_path):\n",
    "    word_counts = defaultdict(int)\n",
    "\n",
    "    for line in read_book_line_by_line(file_path):\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            word_counts[word.lower()] += 1\n",
    "\n",
    "    return word_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our functions set up, we can use them to analyze \"War and Peace\" (or any other large text file) and get the word frequencies. First, download the text file of \"War and Peace\" from a source like Project Gutenberg and save it to your local machine. Then, call the `count_words()` function with the file path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/shakespeare.txt\"\n",
    "word_counts = count_words(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now access the word frequencies using the `word_counts` dictionary. For example, you can print the 10 most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 27549\n",
      "and: 26037\n",
      "i: 19540\n",
      "to: 18700\n",
      "of: 18010\n",
      "a: 14383\n",
      "my: 12455\n",
      "in: 10671\n",
      "you: 10630\n",
      "that: 10487\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "sorted_word_counts = sorted(word_counts.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "for word, count in sorted_word_counts[:10]:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
